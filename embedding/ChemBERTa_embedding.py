# file: chemberta_embedding_extractor.py
import os
import argparse
import numpy as np
import pandas as pd
import torch
from tqdm.auto import tqdm
from transformers import AutoTokenizer, AutoModel
from rdkit import Chem
import warnings

# æŠ‘åˆ¶ä¸å¿…è¦çš„è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, module="transformers")


def load_model_and_tokenizer(model_dir: str, device: torch.device, local_only: bool = True):
    """ç§‘ç ”çº§æ¨¡å‹åŠ è½½ - æ”¯æŒå¿«é€Ÿåˆ†è¯å™¨å’Œå®Œå…¨ç¦»çº¿æ¨¡å¼"""
    try:
        tok = AutoTokenizer.from_pretrained(
            model_dir,
            use_fast=True,  # å¯ç”¨å¿«é€Ÿåˆ†è¯å™¨
            local_files_only=local_only,
            trust_remote_code=False  # å®‰å…¨æ¨¡å¼
        )
        mdl = AutoModel.from_pretrained(
            model_dir,
            local_files_only=local_only,
            trust_remote_code=False
        ).to(device).eval()
        return tok, mdl
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        raise


def canonicalize_smiles(smi: str, drop_stereo: bool = False):
    """
    ç§‘ç ”çº§SMILESè§„èŒƒåŒ–
    éµå¾ªRDKitæœ€ä½³å®è·µï¼Œä¿ç•™ç«‹ä½“åŒ–å­¦ä¿¡æ¯ï¼ˆé»˜è®¤ï¼‰
    """
    if not isinstance(smi, str) or not smi.strip():
        return None
    try:
        mol = Chem.MolFromSmiles(smi)
        if mol is None:
            return None
        # ä¿ç•™ç«‹ä½“åŒ–å­¦ä¿¡æ¯ï¼ˆé™¤éæ˜ç¡®è¦æ±‚å»é™¤ï¼‰
        return Chem.MolToSmiles(mol, canonical=True, isomericSmiles=not drop_stereo)
    except Exception as e:
        print(f"Error canonicalizing {smi}: {str(e)}")
        return None


def load_smiles_file(input_path):
    """ç§‘ç ”çº§SMILESåŠ è½½ - æ”¯æŒå¤šç§æ ¼å¼å¹¶è¿‡æ»¤æ— æ•ˆè¡Œ"""
    if input_path.lower().endswith('.smi'):
        with open(input_path, 'r', encoding='utf-8', errors='ignore') as f:
            lines = [l.strip() for l in f]
        # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Šè¡Œ
        return [l for l in lines if l and not l.startswith('#')]
    elif input_path.lower().endswith((".xlsx", ".xls")):
        df = pd.read_excel(input_path)
        assert 'SMILES' in df.columns, "Excelæ–‡ä»¶å¿…é¡»åŒ…å«SMILESåˆ—"
        return df['SMILES'].astype(str).tolist()
    else:  # CSV/TSV
        try:
            df = pd.read_csv(input_path, sep=None, engine='python', on_bad_lines='warn')
            assert 'SMILES' in df.columns, "CSVæ–‡ä»¶å¿…é¡»åŒ…å«SMILESåˆ—"
            return df['SMILES'].astype(str).tolist()
        except Exception as e:
            raise ValueError(f"æ–‡ä»¶è§£æå¤±è´¥: {str(e)}")


@torch.inference_mode()
def extract_embeddings(smiles_list, tokenizer, model, device,
                       batch_size=256, max_length=256,
                       l2norm=False):
    """
    ä¿®æ”¹åçš„åµŒå…¥æå–å‡½æ•° - ä»…æå–æœ€åä¸€å±‚çš„ [CLS] æ ‡è®°åµŒå…¥
    ç¬¦åˆé«˜æ°´å¹³SCIè®ºæ–‡æ ‡å‡†
    """
    model.eval()

    # è·å–æ¨¡å‹é…ç½®ä¿¡æ¯
    hidden_size = model.config.hidden_size

    # é¢„åˆ†é…å†…å­˜
    num_samples = len(smiles_list)
    embeddings = np.zeros((num_samples, hidden_size), dtype=np.float32)
    valid_mask = np.zeros(num_samples, dtype=bool)

    # è¿›åº¦æ¡è®¾ç½®
    pbar = tqdm(total=num_samples, desc="æå–åµŒå…¥", unit="smiles")

    i = 0
    while i < num_samples:
        batch_smiles = smiles_list[i:i + batch_size]
        current_batch_size = len(batch_smiles)

        try:
            # æ‰¹å¤„ç†ç¼–ç 
            inputs = tokenizer(
                batch_smiles,
                padding=True,
                truncation=True,
                max_length=max_length,
                return_tensors="pt",
                add_special_tokens=True  # ç¡®ä¿æ·»åŠ  [CLS] å’Œ [SEP] æ ‡è®°
            ).to(device)

            # FP16åŠ é€Ÿ (å¦‚æœå¯ç”¨)
            with torch.cuda.amp.autocast(enabled=device.type == 'cuda'):
                outputs = model(**inputs, output_hidden_states=True)

            # ç§‘ç ”çº§æå–ç­–ç•¥: ä»…ä½¿ç”¨æœ€åä¸€å±‚çš„ [CLS] æ ‡è®°
            # è·å–æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€
            hidden_states = outputs.hidden_states

            # æå–æœ€åä¸€å±‚çš„éšè—çŠ¶æ€
            last_layer_hidden_states = hidden_states[-1]

            # æå–æ¯ä¸ªåºåˆ—çš„ [CLS] æ ‡è®° (ç´¢å¼•ä¸º0)
            cls_embeddings = last_layer_hidden_states[:, 0, :]

            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            batch_embeddings = cls_embeddings.cpu().numpy()

            # L2å½’ä¸€åŒ– (å¯é€‰)
            if l2norm:
                norms = np.linalg.norm(batch_embeddings, axis=1, keepdims=True)
                batch_embeddings = batch_embeddings / (norms + 1e-12)

            # å¡«å……ç»“æœ
            embeddings[i:i + current_batch_size] = batch_embeddings
            valid_mask[i:i + current_batch_size] = True

            # æˆåŠŸå¤„ç†ï¼Œç§»åŠ¨åˆ°ä¸‹ä¸€æ‰¹
            i += current_batch_size
            pbar.update(current_batch_size)

            # æ¸…ç†æ˜¾å­˜ (ä¸é¢‘ç¹æ‰§è¡Œ)
            if device.type == 'cuda' and i % (10 * batch_size) == 0:
                torch.cuda.empty_cache()

        except torch.cuda.OutOfMemoryError:
            # OOMå¤„ç†ï¼šå‡å°æ‰¹å¤§å°å¹¶é‡è¯•
            if batch_size > 1:
                new_batch_size = max(1, batch_size // 2)
                print(f"âš ï¸ OOMè­¦å‘Š: æ‰¹å¤§å°ä»{batch_size}å‡å°åˆ°{new_batch_size}")
                batch_size = new_batch_size
                if device.type == 'cuda':
                    torch.cuda.empty_cache()
            else:
                raise RuntimeError("æ‰¹å¤§å°å·²å‡å°åˆ°1ä½†ä»OOMï¼Œè¯·æ£€æŸ¥æ¨¡å‹æˆ–æ•°æ®")

    pbar.close()
    return embeddings[valid_mask], [s for s, v in zip(smiles_list, valid_mask) if v]


def main():
    parser = argparse.ArgumentParser(
        description="ç§‘ç ”çº§ChemBERTaåµŒå…¥æå–å·¥å…·\nç¬¦åˆé«˜æ°´å¹³SCIè®ºæ–‡æ ‡å‡†",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # æ•°æ®å‚æ•°
    parser.add_argument("--input", required=True,
                        help="è¾“å…¥æ–‡ä»¶(.smi/.csv/.xlsx)")
    parser.add_argument("--output", default="chemberta_embeddings",
                        help="è¾“å‡ºæ–‡ä»¶å‰ç¼€")

    # æ¨¡å‹å‚æ•°
    parser.add_argument("--model", choices=["zinc"], default="zinc",
                        help="é€‰æ‹©æ¨¡å‹: zinc(seyonec) [è„šæœ¬å·²ä¿®æ”¹ä¸ºä»…æ”¯æŒæ­¤é€‰é¡¹]")
    parser.add_argument("--zinc_dir", required=True,
                        help="æœ¬åœ° seyonec/ChemBERTa-zinc-base-v1 ç›®å½•")

    # ç§‘å­¦å¤„ç†å‚æ•°
    parser.add_argument("--batch_size", type=int, default=256,
                        help="åˆå§‹æ‰¹å¤„ç†å¤§å°(è‡ªåŠ¨è°ƒæ•´)")
    parser.add_argument("--max_length", type=int, default=256,
                        help="SMILESæœ€å¤§é•¿åº¦")
    parser.add_argument("--device", choices=["auto", "cpu", "cuda"], default="auto",
                        help="è®¡ç®—è®¾å¤‡")

    # ç¦»çº¿/è”ç½‘æ§åˆ¶
    parser.add_argument("--online", action="store_true",
                        help="å¯ç”¨åœ¨çº¿æ¨¡å¼(é»˜è®¤å®Œå…¨ç¦»çº¿)")

    # åŒ–å­¦å‚æ•°
    parser.add_argument("--drop_stereo", action="store_true",
                        help="å»é™¤ç«‹ä½“åŒ–å­¦(é»˜è®¤ä¿ç•™)")

    # åµŒå…¥æå–å‚æ•°
    parser.add_argument("--l2norm", action="store_true",
                        help="L2å½’ä¸€åŒ–åµŒå…¥å‘é‡")

    args = parser.parse_args()

    # ç§‘ç ”çº§ç¯å¢ƒé…ç½®
    if not args.online:
        os.environ["HF_HUB_OFFLINE"] = "1"  # å¼ºåˆ¶ç¦»çº¿æ¨¡å¼
        os.environ["TRANSFORMERS_OFFLINE"] = "1"

    # è®¾å¤‡é…ç½®
    if args.device == "auto":
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device(args.device)

    print("=" * 60)
    print(f"ğŸ§ª ç§‘ç ”çº§ChemBERTaåµŒå…¥æå–å·¥å…·")
    print("=" * 60)
    print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {args.input}")
    print(f"ğŸ’¾ è¾“å‡ºå‰ç¼€: {args.output}")
    print(f"âš™ï¸  è®¾å¤‡: {device} | åˆå§‹æ‰¹å¤§å°: {args.batch_size}")
    print(f"ğŸ”¬ æå–ç­–ç•¥: æœ€åä¸€å±‚ [CLS] æ ‡è®° | L2å½’ä¸€åŒ–={args.l2norm}")
    print(f"ğŸ§ª åŒ–å­¦å¤„ç†: ä¿ç•™ç«‹ä½“åŒ–å­¦={not args.drop_stereo}")
    print(f"ğŸŒ è”ç½‘æ¨¡å¼: {'å¯ç”¨' if args.online else 'ç¦ç”¨'}")
    print("=" * 60)

    # åŠ è½½å¹¶è§„èŒƒåŒ–SMILES
    print("\nğŸ” åŠ è½½å¹¶é¢„å¤„ç†SMILES...")
    raw_smiles = load_smiles_file(args.input)
    print(f"  åŠ è½½SMILESæ•°é‡: {len(raw_smiles)}")

    canon_smiles = []
    invalid_indices = []
    for i, smi in enumerate(tqdm(raw_smiles, desc="è§„èŒƒåŒ–SMILES")):
        canon = canonicalize_smiles(smi, args.drop_stereo)
        if canon is None:
            invalid_indices.append(i)
        else:
            canon_smiles.append(canon)

    print(f"âœ… æœ‰æ•ˆSMILES: {len(canon_smiles)} | âŒ æ— æ•ˆ: {len(invalid_indices)}")

    # å¤„ç†æ— æ•ˆSMILES
    if invalid_indices:
        invalid_df = pd.DataFrame({
            "original_index": invalid_indices,
            "original_smiles": [raw_smiles[i] for i in invalid_indices]
        })
        invalid_path = f"{args.output}_invalid_smiles.csv"
        invalid_df.to_csv(invalid_path, index=False)
        print(f"âš ï¸ ä¿å­˜æ— æ•ˆSMILESåˆ°: {invalid_path}")

    # ========= æ¨¡å‹å¤„ç† =========
    # ä»…ä½¿ç”¨ seyonec/ChemBERTa-zinc-base-v1 æ¨¡å‹
    tasks = [("zinc", args.zinc_dir)]

    # æ˜¾ç¤ºè­¦å‘Šå¦‚æœç”¨æˆ·å°è¯•ä½¿ç”¨å…¶ä»–æ¨¡å‹
    if args.model != "zinc":
        print(f"âš ï¸  è­¦å‘Š: è„šæœ¬å·²ä¿®æ”¹ä¸ºä»…ä½¿ç”¨ 'zinc' æ¨¡å‹ï¼Œå¿½ç•¥ '{args.model}' é€‰é¡¹")

    for tag, model_dir in tasks:
        print("\n" + "=" * 60)
        print(f"ğŸš€ å¤„ç†æ¨¡å‹: {tag.upper()} | è·¯å¾„: {model_dir}")
        print("=" * 60)

        try:
            tokenizer, model = load_model_and_tokenizer(
                model_dir, device, local_only=not args.online
            )

            # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
            print(f"ğŸ”§ æ¨¡å‹åç§°: {model.config._name_or_path}")
            print(f"ğŸ”§ éšè—å±‚å¤§å°: {model.config.hidden_size}")
            print(f"ğŸ”§ æ€»å±‚æ•°: {model.config.num_hidden_layers}")
            print(f"ğŸ”§ æå–ç­–ç•¥: æœ€åä¸€å±‚ [CLS] æ ‡è®°")

            # æå–åµŒå…¥
            embeddings, valid_smiles = extract_embeddings(
                canon_smiles, tokenizer, model, device,
                batch_size=args.batch_size,
                max_length=args.max_length,
                l2norm=args.l2norm
            )

            # ä¿å­˜ç»“æœ
            emb_path = f"{args.output}_{tag}.npy"
            meta_path = f"{args.output}_{tag}_metadata.csv"

            np.save(emb_path, embeddings)
            meta_df = pd.DataFrame({
                "original_index": [i for i, s in enumerate(raw_smiles)
                                   if canonicalize_smiles(s, args.drop_stereo) in valid_smiles],
                "original_smiles": [s for i, s in enumerate(raw_smiles)
                                   if canonicalize_smiles(s, args.drop_stereo) in valid_smiles],
                "canonical_smiles": valid_smiles
            })
            meta_df.to_csv(meta_path, index=False)

            print("\nâœ… å®Œæˆ!")
            print(f"  åµŒå…¥ç»´åº¦: {embeddings.shape}")
            print(f"  åµŒå…¥æ–‡ä»¶: {emb_path}")
            print(f"  å…ƒæ•°æ®æ–‡ä»¶: {meta_path}")
            print(f"  åµŒå…¥ç»Ÿè®¡: å‡å€¼={np.mean(embeddings):.4f} Â± {np.std(embeddings):.4f}")

        except Exception as e:
            print(f"\nâŒ å¤„ç†æ¨¡å‹ {tag} æ—¶å‡ºé”™: {str(e)}")
            continue


if __name__ == "__main__":
    main()